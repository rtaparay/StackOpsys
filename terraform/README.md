# Terraform Module for Kubernetes Cluster on Proxmox

A comprehensive Terraform module for creating and managing a Kubernetes cluster on Proxmox Virtual Environment using Ubuntu 22.04. This project includes complete automation with Packer, Terraform, and Ansible, plus GitHub Actions integration for CI/CD.

---

## ğŸ“‹ Table of Contents

- Project Architecture
- Prerequisites
- Project Structure
- Configuration
- Installation and Usage
- Modules
- Environments
- Kubernetes Tools
- CI/CD with GitHub Actions
- Outputs
- Troubleshooting
## ğŸ—ï¸ Project Architecture
This project implements a complete infrastructure for Kubernetes using:

- Packer : Creates base VM templates with Ubuntu 22.04
- Terraform : Provisions infrastructure on Proxmox
- Ansible : Post-deployment configuration
- Kubernetes : Cluster with master and worker nodes
- Additional Tools : Longhorn (storage), NGINX Ingress Controller
## ğŸ“‹ Prerequisites
### Required Software
- Proxmox Virtual Environment 8.3.3+
- Terraform v1.8+
- Packer
- Ansible
- kubectl
### Proxmox Configuration
1. User and permissions : Create user with appropriate permissions for Terraform
2. API Token : Generate API token for authentication
3. Base template : Create template with Packer (reference: directory:9001/base-9001-disk-0.qcow2 )
4. Network : Configure network bridge ( vmbr0 by default)
5. Storage : Configure datastore ( directory by default)

### Network Configuration
- Gateway : 192.168.100.1
- IP Range : 192.168.100.220-224
- CIDR : /24
- DNS : Configure according to your environment
## ğŸ“ Project Structure
```
â”œâ”€â”€Â .github/
â”‚Â Â Â â””â”€â”€Â workflows/
â”‚Â Â Â Â Â Â Â â””â”€â”€Â terraform.yamlÂ Â Â Â Â Â Â Â Â Â #Â CI/CDÂ pipeline
â”œâ”€â”€Â environments/Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â EnvironmentÂ configurations
â”‚Â Â Â â”œâ”€â”€Â dev.tfvarsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â Development
â”‚Â Â Â â”œâ”€â”€Â prd.tfvarsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â Production
â”‚Â Â Â â”œâ”€â”€Â qas.tfvarsÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â QA/Testing
â”œâ”€â”€Â modules/
â”‚Â Â Â â”œâ”€â”€Â vms_proxmox/Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â MainÂ VMÂ module
â”‚Â Â Â â”‚Â Â Â â”œâ”€â”€Â main.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â MainÂ resources
â”‚Â Â Â â”‚Â Â Â â”œâ”€â”€Â variables.tfÂ Â Â Â Â Â Â Â Â Â Â Â #Â ModuleÂ variables
â”‚Â Â Â â”‚Â Â Â â”œâ”€â”€Â outputs.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â #Â ModuleÂ outputs
â”‚Â Â Â â”‚Â Â Â â”œâ”€â”€Â providers.tfÂ Â Â Â Â Â Â Â Â Â Â Â #Â Providers
â”‚Â Â Â â”‚Â Â Â â”œâ”€â”€Â local.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â LocalÂ configuration
â”‚Â Â Â â”‚Â Â Â â””â”€â”€Â mapping_pci.tfÂ Â Â Â Â Â Â Â Â Â #Â PCIÂ mappingÂ forÂ GPU
â”‚Â Â Â â””â”€â”€Â tools_k8s/Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â KubernetesÂ tools
â”‚Â Â Â Â Â Â Â â”œâ”€â”€Â longhorn.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â #Â DistributedÂ storage
â”‚Â Â Â Â Â Â Â â”œâ”€â”€Â nginx.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â IngressÂ controller
â”‚Â Â Â Â Â Â Â â”œâ”€â”€Â reloader.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â #Â ConfigÂ reloader
â”‚Â Â Â Â Â Â Â â””â”€â”€Â versions.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â #Â ProviderÂ versions
â”œâ”€â”€Â output/Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â GeneratedÂ files
â”‚Â Â Â â””â”€â”€Â kubeconfig.yamlÂ Â Â Â Â Â Â Â Â Â Â Â #Â kubectlÂ configuration
â”œâ”€â”€Â main.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â MainÂ configuration
â”œâ”€â”€Â variables.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â GlobalÂ variables
â”œâ”€â”€Â outputs.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â MainÂ outputs
â”œâ”€â”€Â providers.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â ProviderÂ configuration
â”œâ”€â”€Â versions.tfÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â #Â RequiredÂ versions
â””â”€â”€Â prepare-release-tfdocs.shÂ Â Â Â Â Â Â #Â DocumentationÂ script
```
## âš™ï¸ Configuration
### 1. Proxmox Variables
Configure Proxmox credentials in your .tfvars file:

```
proxmoxÂ =Â {
Â Â endpointÂ Â Â Â Â Â Â Â Â Â Â =Â "https://192.168.100.100:8006/api2/json"
Â Â insecureÂ Â Â Â Â Â Â Â Â Â Â =Â true
Â Â usernameÂ Â Â Â Â Â Â Â Â Â Â =Â "root"
Â Â passwordÂ Â Â Â Â Â Â Â Â Â Â =Â "your_password"
Â Â api_tokenÂ Â Â Â Â Â Â Â Â Â =Â "root@pam!iac-tf=your_token_here"
Â Â random_vm_idsÂ Â Â Â Â Â =Â true
Â Â random_vm_id_startÂ =Â 300
Â Â random_vm_id_endÂ Â Â =Â 304
}
```
### 2. Cluster Configuration
```
clusterÂ =Â {
Â Â nameÂ Â Â Â Â =Â "cluster-kubeadm"
Â Â gatewayÂ Â =Â "192.168.100.1"
Â Â cidrÂ Â Â Â Â =Â 24
Â Â endpointÂ =Â "192.168.100.220"
}
```
### 3. VM Configuration
```
vmsÂ =Â {
Â Â "k8s-master-01"Â =Â {
Â Â Â Â host_nodeÂ Â Â Â Â Â =Â "pve01"
Â Â Â Â machine_typeÂ Â Â =Â "controlplane"
Â Â Â Â ipÂ Â Â Â Â Â Â Â Â Â Â Â Â =Â "192.168.100.220"
Â Â Â Â cpuÂ Â Â Â Â Â Â Â Â Â Â Â =Â 4
Â Â Â Â ram_dedicatedÂ Â =Â 4096
Â Â Â Â os_disk_sizeÂ Â Â =Â 70
Â Â Â Â data_disk_sizeÂ =Â 70
Â Â Â Â datastore_idÂ Â Â =Â "directory"
Â Â Â Â file_idÂ Â Â Â Â Â Â Â =Â "directory:9001/base-9001-disk-0.qcow2"
Â Â }
Â Â "k8s-worker-01"Â =Â {
Â Â Â Â host_nodeÂ Â Â Â Â Â =Â "pve01"
Â Â Â Â machine_typeÂ Â Â =Â "worker"
Â Â Â Â ipÂ Â Â Â Â Â Â Â Â Â Â Â Â =Â "192.168.100.223"
Â Â Â Â cpuÂ Â Â Â Â Â Â Â Â Â Â Â =Â 4
Â Â Â Â ram_dedicatedÂ Â =Â 8096
Â Â Â Â os_disk_sizeÂ Â Â =Â 70
Â Â Â Â data_disk_sizeÂ =Â 70
Â Â Â Â datastore_idÂ Â Â =Â "directory"
Â Â Â Â file_idÂ Â Â Â Â Â Â Â =Â "directory:9001/base-9001-disk-0.qcow2"
Â Â }
Â Â "k8s-worker-02"Â =Â {
Â Â Â Â host_nodeÂ Â Â Â Â Â =Â "pve01"
Â Â Â Â machine_typeÂ Â Â =Â "worker"
Â Â Â Â ipÂ Â Â Â Â Â Â Â Â Â Â Â Â =Â "192.168.100.224"
Â Â Â Â cpuÂ Â Â Â Â Â Â Â Â Â Â Â =Â 4
Â Â Â Â ram_dedicatedÂ Â =Â 8096
Â Â Â Â os_disk_sizeÂ Â Â =Â 70
Â Â Â Â data_disk_sizeÂ =Â 70
Â Â Â Â datastore_idÂ Â Â =Â "directory"
Â Â Â Â file_idÂ Â Â Â Â Â Â Â =Â "directory:9001/base-9001-disk-0.qcow2"
Â Â }
}
```
## ğŸš€ Installation and Usage

### 1. Initialization

```sh
## CloneÂ theÂ repository
â•°â”€ $ gitÂ cloneÂ https://github.com/rtaparay/StackOpsys
â•°â”€ $ cdÂ terraform
â•°â”€ $ terraform init

### ouput                             
Initializing the backend...
Initializing modules...
- vms_proxmox in modules/vms_proxmox
Initializing provider plugins...
- Reusing previous version of hashicorp/time from the dependency lock file
- Reusing previous version of hashicorp/kubernetes from the dependency lock file
- Reusing previous version of hashicorp/helm from the dependency lock file
- Reusing previous version of hashicorp/local from the dependency lock file
- Reusing previous version of hashicorp/http from the dependency lock file
- Reusing previous version of bpg/proxmox from the dependency lock file
- Installing hashicorp/kubernetes v2.38.0...
- Installed hashicorp/kubernetes v2.38.0 (signed by HashiCorp)
- Installing hashicorp/helm v3.0.2...
- Installed hashicorp/helm v3.0.2 (signed by HashiCorp)
- Installing hashicorp/local v2.5.3...
- Installed hashicorp/local v2.5.3 (signed by HashiCorp)
- Installing hashicorp/http v3.5.0...
- Installed hashicorp/http v3.5.0 (signed by HashiCorp)
- Installing bpg/proxmox v0.81.0...
- Installed bpg/proxmox v0.81.0 (self-signed, key ID F0582AD6AE97C188)
- Installing hashicorp/time v0.13.1...
- Installed hashicorp/time v0.13.1 (signed by HashiCorp)
Partner and community providers are signed by their developers.
If you'd like to know more about provider signing, you can read about it here:
https://developer.hashicorp.com/terraform/cli/plugins/signing

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
```

### 2. Planning
```
#Â ReviewÂ changesÂ toÂ beÂ applied
```sh
â•°â”€ $ terraform plan -var-file=environments/dev.tfvars
### ouput
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-worker-01"]: Refreshing state... [id=303]
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-worker-02"]: Refreshing state... [id=301]
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-master-01"]: Refreshing state... [id=302]

Plan: 0 to add, 3 to change, 0 to destroy.
```

### 3. Application
```sh
#Â ApplyÂ theÂ configuration

â•°â”€ $ terraform apply -var-file=environments/dev.tfvars

### ouput
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-worker-02"]: Modifying... [id=301]
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-worker-01"]: Modifying... [id=303]
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-master-01"]: Modifying... [id=302]

Apply complete! Resources: 4 added, 3 changed, 0 destroyed.

Outputs:

cluster_name = "cluster-kubeadm"
config_ipv4_addresses = {
  "cluster-kubeadm-k8s-master-01" = "192.168.100.220/24"
  "cluster-kubeadm-k8s-worker-01" = "192.168.100.223/24"
  "cluster-kubeadm-k8s-worker-02" = "192.168.100.224/24"
}
qemu_ipv4_addresses = {
  "k8s-master-01" = "192.168.100.220"
  "k8s-worker-01" = "192.168.100.223"
  "k8s-worker-02" = "192.168.100.224"
}
vm_ipv4_address_vms = [
  "192.168.100.220/24",
  "192.168.100.223/24",
  "192.168.100.224/24",
]
```

### 5. Destruction
```sh
#Â DestroyÂ allÂ infrastructure

â•°â”€ $ terraform destroy -var-file=environments/dev.tfvars

### ouput
Do you really want to destroy all resources?
  Terraform will destroy all your managed infrastructure, as shown above.
  There is no undo. Only 'yes' will be accepted to confirm.

  Enter a value: yes

module.vms_proxmox.time_sleep.waiting_if_dhcp: Destroying... [id=2025-08-20T04:37:29Z]
module.vms_proxmox.time_sleep.waiting_if_dhcp: Destruction complete after 0s
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-master-01"]: Destroying... [id=302]
module.vms_proxmox.proxmox_virtual_environment_vm.vms["k8s-worker-01"]: Destroying... [id=303]

Destroy complete! Resources: 4 destroyed.
```

## ğŸ“¦ Modules
### vms_proxmox Module
Purpose : Create and manage virtual machines on Proxmox

Features :

- VM creation with custom configuration
- Support for multiple node types (controlplane, worker)
- Static network configuration
- Separate system and data disks
- PCI mapping support (GPU passthrough)
- Automatic tags for organization
Resources created :

- proxmox_virtual_environment_vm.vms : Main VMs
- time_sleep.waiting_if_dhcp : Wait for DHCP configuration
- proxmox_virtual_environment_hardware_mapping_pci.pci : Optional PCI mapping
### tools_k8s Module
Purpose : Install additional tools on the Kubernetes cluster

Tools included :
 Longhorn (Storage)
- Version : v1.9.0 (configurable)
- Namespace : longhorn-system
- Function : Distributed storage for Kubernetes
- Configuration : Data path at /var/mnt/longhorn NGINX Ingress Controller
- Version : 4.12.2
- Namespace : ingress-nginx
- Function : HTTP/HTTPS traffic ingress controller
- Configuration : DaemonSet with fixed IP (192.168.100.104) Reloader
- Function : Automatic pod restart when ConfigMaps/Secrets change
## ğŸŒ Environments
The project supports multiple environments via .tfvars files:

- dev.tfvars : Development environment (3 nodes)
- stg.tfvars : Staging environment
- qas.tfvars : QA/testing environment
- prd.tfvars : Production environment
Each environment can have:

- Different resource configurations (CPU, RAM, disk)
- Different networks and IP ranges
- Specific tool configurations
## ğŸ› ï¸ Kubernetes Tools
### Enabling Tools
To enable tools, uncomment and configure in your .tfvars file:

```yaml
#Â DistributedÂ storage
longhorn_enabledÂ =Â true
longhorn_versionÂ =Â "v1.9.0"

#Â IngressÂ controller
ingress_nginx_enabledÂ =Â true
```
### Accessing Longhorn UI
```
#Â PortÂ forwardÂ toÂ accessÂ UI
kubectlÂ port-forwardÂ -nÂ longhorn-systemÂ svc/longhorn-frontendÂ 8080:80
#Â AccessÂ http://localhost:8080
```
### Ingress Configuration
```yaml
apiVersion:Â networking.k8s.io/v1
kind:Â Ingress
metadata:
Â Â name:Â example-ingress
Â Â annotations:
Â Â Â Â kubernetes.io/ingress.class:Â nginx
spec:
Â Â rules:
Â Â -Â host:Â example.local
Â Â Â Â http:
Â Â Â Â Â Â paths:
Â Â Â Â Â Â -Â path:Â /
Â Â Â Â Â Â Â Â pathType:Â Prefix
Â Â Â Â Â Â Â Â backend:
Â Â Â Â Â Â Â Â Â Â service:
Â Â Â Â Â Â Â Â Â Â Â Â name:Â example-service
Â Â Â Â Â Â Â Â Â Â Â Â port:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â number:Â 80
```
## ğŸ”„ CI/CD with GitHub Actions
The project includes a complete CI/CD pipeline:

### Secret Configuration
In your GitHub repository, configure these secrets:

```yaml
PROXMOX_VE_SSH_USERNAME=root
PROXMOX_VE_SSH_PASSWORD=your_password
PROXMOX_VE_API_TOKEN=your_api_token
TF_VAR_GITHUB_PAT=your_github_token
```
### Environment Variables
```yaml
PROXMOX_VE_ENDPOINT=https://your-proxmox:8006/api2/json
PROXMOX_VE_INSECURE=true
```
### Pipeline Steps
1. Checkout : Download code
2. Setup Terraform : Install Terraform
3. Init : Initialize project
4. Format Check : Verify code format
5. Validate : Validate configuration
6. Plan : Generate execution plan
7. Apply : Apply changes (main branch only)

## ğŸ“¤ Outputs
The project generates several useful outputs:

```
#Â ListÂ ofÂ allÂ VMÂ IPs
vm_ipv4_address_vmsÂ =Â [
Â Â "192.168.100.220/24",
Â Â "192.168.100.223/24",
Â Â "192.168.100.224/24"
]

#Â VMÂ nameÂ toÂ IPÂ mapping
config_ipv4_addressesÂ =Â {
Â Â "cluster-kubeadm-k8s-master-01"Â =Â "192.168.100.220/24"
Â Â "cluster-kubeadm-k8s-worker-01"Â =Â "192.168.100.223/24"
Â Â "cluster-kubeadm-k8s-worker-02"Â =Â "192.168.100.224/24"
}

#Â QEMUÂ IPsÂ (withoutÂ clusterÂ prefix)
qemu_ipv4_addressesÂ =Â {
Â Â "k8s-master-01"Â =Â "192.168.100.220"
Â Â "k8s-worker-01"Â =Â "192.168.100.223"
Â Â "k8s-worker-02"Â =Â "192.168.100.224"
}

#Â ClusterÂ name
cluster_nameÂ =Â "cluster-kubeadm"
```
## ğŸ”§ Troubleshooting
### Common Issues 1. Proxmox Authentication Error
```
#Â VerifyÂ connectivity
curlÂ -kÂ https://192.168.100.100:8006/api2/json/version

#Â VerifyÂ token
curlÂ -kÂ -HÂ "Authorization:Â PVEAPIToken=root@pam!iac-tf=your_token"Â \
Â Â https://192.168.100.100:8006/api2/json/version
``` 2. Template Not Found
```

# ListÂ availableÂ templates
```
pveshÂ getÂ /nodes/pve01/storage/directory/contentÂ --contentÂ vztmpl
```
# VerifyÂ templateÂ exists
```
pveshÂ getÂ /nodes/pve01/qemu/9001/config
```
# Network Issues
```
# VerifyÂ bridgeÂ configuration
ipÂ linkÂ showÂ vmbr0
```
#Â VerifyÂ nodeÂ status
```
kubectlÂ getÂ nodesÂ -oÂ wide
```
#Â CheckÂ kubeletÂ logs
```
sshÂ user@192.168.100.220Â "sudoÂ journalctlÂ -uÂ kubeletÂ -f"
```
### Useful Commands
```
#Â ViewÂ TerraformÂ state
terraformÂ show

#Â ViewÂ outputs
terraformÂ output

#Â RefreshÂ state
terraformÂ refreshÂ -var-file=environments/dev.tfvars
```

### Logging and Debugging
```
#Â EnableÂ detailedÂ TerraformÂ logs
exportÂ TF_LOG=DEBUG
terraformÂ applyÂ -var-file=environments/dev.tfvars

#Â VerifyÂ kubeconfigÂ configuration
kubectlÂ configÂ viewÂ --kubeconfig=output/kubeconfig.yaml
```
## ğŸ“š Additional Resources
- Proxmox Documentation
- Terraform Proxmox Provider
- Kubernetes Documentation
- Longhorn Documentation
- NGINX Ingress Controller

## ğŸ¤ Contributing
1. Fork the project
2. Create a feature branch ( git checkout -b feature/new-functionality )
3. Commit your changes ( git commit -am 'Add new functionality' )
4. Push to the branch ( git push origin feature/new-functionality )
5. Create a Pull Request

## ğŸ“„ License
This project is licensed under the MIT License. See the LICENSE file for details.